{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# using python's nltk toolkit and numpy\n",
    "import nltk\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "import os\n",
    "import json\n",
    "import datetime\n",
    "stemmer = LancasterStemmer()\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43 sentences of training data\n"
     ]
    }
   ],
   "source": [
    "#2 classes of training data\n",
    "training_data = []\n",
    "training_data.append({\"class\":\"eat\", \"sentence\":\"how about a lunch?\"})\n",
    "training_data.append({\"class\":\"eat\", \"sentence\":\"up for a snack?\"})\n",
    "training_data.append({\"class\":\"eat\", \"sentence\":\"let's go out for a lunch\"})\n",
    "training_data.append({\"class\":\"eat\", \"sentence\":\"let's go for a bite\"})\n",
    "training_data.append({\"class\":\"eat\",\"sentence\":\"Is it noon already? I want eat something\"})\n",
    "training_data.append({\"class\":\"eat\",\"sentence\":\"Did you eat lunch today\"})\n",
    "training_data.append({\"class\":\"eat\",\"sentence\":\"Do you like to eat beef?\"})\n",
    "training_data.append({\"class\":\"eat\",\"sentence\":\"Do you like fruits?\"})\n",
    "training_data.append({\"class\":\"eat\",\"sentence\":\"Do you eat lunch at school?\"})\n",
    "training_data.append({\"class\":\"eat\",\"sentence\":\"Do you like to eat rice?\"})\n",
    "training_data.append({\"class\":\"eat\",\"sentence\":\"Do you bring lunch to the school?\"})\n",
    "training_data.append({\"class\":\"eat\",\"sentence\":\"Do you like Thai food?\"})\n",
    "training_data.append({\"class\":\"eat\",\"sentence\":\"How about Chinese food?\"})\n",
    "training_data.append({\"class\":\"eat\",\"sentence\":\"Will you say yes for Spanish food?\"})\n",
    "training_data.append({\"class\":\"eat\",\"sentence\":\"Shall we have some French food?\"})\n",
    "training_data.append({\"class\":\"eat\",\"sentence\":\"I love Italian food, how about you?\"})\n",
    "training_data.append({\"class\":\"eat\",\"sentence\":\"How about indian food?\"})\n",
    "training_data.append({\"class\":\"eat\",\"sentence\":\"Lets have mexican food\"})\n",
    "training_data.append({\"class\":\"eat\",\"sentence\":\"Are you up for some wings?\"})\n",
    "training_data.append({\"class\":\"eat\",\"sentence\":\"Would you like to have some food?\"})\n",
    "training_data.append({\"class\":\"eat\",\"sentence\":\"would you like to go for a dinner today?\"})\n",
    "training_data.append({\"class\":\"eat\",\"sentence\":\"What's your favorite junk food?\"})\n",
    "\n",
    "training_data.append({\"class\":\"noeat\",\"sentence\":\"Indian food is very spicy. I don't like spicy food\"})\n",
    "training_data.append({\"class\":\"noeat\",\"sentence\":\"Most people hate mexican food because it is spicy\"})\n",
    "training_data.append({\"class\":\"noeat\",\"sentence\":\"I really much preference toward spanish cuisine\"})\n",
    "training_data.append({\"class\":\"noeat\",\"sentence\":\"It is not possible now\"})\n",
    "training_data.append({\"class\":\"noeat\",\"sentence\":\"I have some urgent business\"})\n",
    "training_data.append({\"class\":\"noeat\",\"sentence\":\"I'm in a meeting now.\"})\n",
    "training_data.append({\"class\":\"noeat\",\"sentence\":\"I'm not in a mood to eat\"})\n",
    "training_data.append({\"class\":\"noeat\",\"sentence\":\"I don't like chinese food\"})\n",
    "training_data.append({\"class\":\"noeat\",\"sentence\":\"I don't like french cuisine\"})\n",
    "training_data.append({\"class\":\"noeat\",\"sentence\":\"Italian cuisine is something I really hate\"})\n",
    "training_data.append({\"class\":\"noeat\", \"sentence\":\"i got some work to do\"})\n",
    "training_data.append({\"class\":\"noeat\", \"sentence\":\"i'm not hungry\"})\n",
    "training_data.append({\"class\":\"noeat\", \"sentence\":\"i'm full, someother time\"})\n",
    "training_data.append({\"class\":\"noeat\", \"sentence\":\"not now\"})\n",
    "training_data.append({\"class\":\"noeat\", \"sentence\":\"lets not eat\"})\n",
    "training_data.append({\"class\":\"noeat\", \"sentence\":\"I don't feel like eating\"})\n",
    "training_data.append({\"class\":\"noeat\", \"sentence\":\"I'm not hungry anymore\"})\n",
    "training_data.append({\"class\":\"noeat\", \"sentence\":\"I don't want to eat at this time\"})\n",
    "training_data.append({\"class\":\"noeat\", \"sentence\":\"I don't have appetite\"})\n",
    "training_data.append({\"class\":\"noeat\", \"sentence\":\"I am full right\"})\n",
    "training_data.append({\"class\":\"noeat\", \"sentence\":\"Sorry, I'm on a fast\"})\n",
    "\n",
    "\n",
    "# cross validation data set\n",
    "validation_data = []\n",
    "\n",
    "validation_data.append({\"class\":\"noeat\",\"sentence\":\"I dont like to eat right now\"})\n",
    "validation_data.append({\"class\":\"noeat\",\"sentence\":\"I'm not in the mood to eat\"})\n",
    "validation_data.append({\"class\":\"noeat\",\"sentence\":\"I have no intention of eating\"})\n",
    "validation_data.append({\"class\":\"noeat\",\"sentence\":\"I just had my stomach full, so can't eat anything\"})\n",
    "validation_data.append({\"class\":\"noeat\",\"sentence\":\"I have other tasks to do, sorry\"})\n",
    "\n",
    "validation_data.append({\"class\":\"eat\",\"sentence\":\"I heard a new lunch place, what say?\"})\n",
    "validation_data.append({\"class\":\"eat\",\"sentence\":\"I would eat anything you feed\"})\n",
    "validation_data.append({\"class\":\"eat\",\"sentence\":\"I'm super hungry right now\"})\n",
    "validation_data.append({\"class\":\"eat\",\"sentence\":\"I would like to eat burrito\"})\n",
    "validation_data.append({\"class\":\"eat\",\"sentence\":\"How about some snacks with coffee\"})\n",
    "\n",
    "print (\"%s sentences of training data\" % len(training_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43 documents\n",
      "2 classes ['eat', 'noeat']\n",
      "94 unique stemmed words ['ind', 'someoth', 'bit', 'hav', 'spicy', 'tha', 'hat', 'it', 'now', 'snack', 'much', 'thi', 'work', 'up', 'junk', 'din', 'mex', 'in', 'at', ',', 'cuisin', 'got', 'the', 'poss', 'not', \"'m\", 'feel', 'want', 'bring', 'lik', 'noon', 'someth', 'pref', 'would', 'what', 'how', 'today', 'chines', 'wil', 'am', 'lov', 'is', 'meet', 'wing', 'tim', 'fruit', 'very', 'did', 'becaus', 'eat', 'ar', 'mood', 'urg', 'ric', 'som', 'out', 'yo', 'you', '.', 'ful', 'fast', \"n't\", 'food', 'beef', 'already', 'most', 'lunch', 'hungry', 'about', 'shal', 'french', 'to', 'go', 'right', 'favorit', 'span', 'we', 'do', 'let', 'school', \"'s\", 'toward', 'busy', 'say', 'for', 'sorry', 'a', 'ye', 'appetit', 'i', 'real', 'on', 'anym', 'peopl']\n"
     ]
    }
   ],
   "source": [
    "words = []\n",
    "classes = []\n",
    "documents = []\n",
    "ignore_words = ['?']\n",
    "# loop through each sentence in our training data and tokenize them with space as delimeter\n",
    "# add all words, classes \n",
    "# add all the documents to our existing corpus\n",
    "# stem all the words, convert them to lower case and remove duplicates\n",
    "for pattern in training_data:\n",
    "    w = nltk.word_tokenize(pattern['sentence'])\n",
    "    words.extend(w)\n",
    "    documents.append((w, pattern['class']))\n",
    "    if pattern['class'] not in classes:\n",
    "        classes.append(pattern['class'])\n",
    "\n",
    "words = [stemmer.stem(w.lower()) for w in words if w not in ignore_words]\n",
    "words = list(set(words))\n",
    "\n",
    "classes = list(set(classes))\n",
    "\n",
    "print (len(documents), \"documents\")\n",
    "print (len(classes), \"classes\", classes)\n",
    "print (len(words), \"unique stemmed words\", words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# this module is about creating our training data from sentences above\n",
    "# iterate over each data point and apply stemming and bag of words\n",
    "# the output is based on the classification value, 1 = eat and 0 = noeat\n",
    "# bag of words approach is used in this model generation\n",
    "training = []\n",
    "output = []\n",
    "output_empty = [0] * len(classes)\n",
    "\n",
    "for doc in documents:\n",
    "    bag = []\n",
    "    pattern_words = doc[0]\n",
    "    pattern_words = [stemmer.stem(word.lower()) for word in pattern_words]\n",
    "    for w in words:\n",
    "        bag.append(1) if w in pattern_words else bag.append(0)\n",
    "\n",
    "    training.append(bag)\n",
    "    output_row = list(output_empty)\n",
    "    output_row[classes.index(doc[1])] = 1\n",
    "    output.append(output_row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# compute sigmoid function\n",
    "# compute derivative of sigmoid function\n",
    "\n",
    "def sigmoid(x):\n",
    "    output = 1/(1+np.exp(-x))\n",
    "    return output\n",
    "\n",
    "def sigmoid_output_to_derivative(output):\n",
    "    return output*(1-output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# 1st part in learning is to modify the incoming data into required format\n",
    "# tokenize the words and stem them\n",
    "# return bag of words array: 0 or 1 for each word in the bag that exists in the sentence\n",
    "# returned array contains all our corpus in the array with 0s and 1s depending on the existance of the word in corpus\n",
    "\n",
    "def feature(sentence, words, show_details=False):\n",
    "    sentence_words = nltk.word_tokenize(sentence)\n",
    "    sentence_words = [stemmer.stem(word.lower()) for word in sentence_words]\n",
    "    bag = [0]*len(words)  \n",
    "    for s in sentence_words:\n",
    "        for i,w in enumerate(words):\n",
    "            if w == s: \n",
    "                bag[i] = 1\n",
    "                if show_details:\n",
    "                    print (\"found in bag: %s\" % w)\n",
    "\n",
    "    return(np.array(bag))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# convert the sentence into the way we want, covert the sentence into bag of words\n",
    "# get 1s and 0s based on the words available in our corpus\n",
    "# x is the input layer, \n",
    "# l1 is a hidden layer which performs dot product of input sentence and the layer1 weight values\n",
    "# l2 is the output layer which performs dot product of output of layer1 and the weight values\n",
    "# l2 returns the classifier 1 or 0\n",
    "\n",
    "def predictusing_ann(sentence, show_details=False):\n",
    "\n",
    "    x = feature(sentence.lower(), words, show_details)\n",
    "    if show_details:\n",
    "        print (\"sentence:\", sentence, \"\\n feature set:\", x)\n",
    "    l0 = x\n",
    "    l1 = sigmoid(np.dot(l0, weights_0))\n",
    "    l2 = sigmoid(np.dot(l1, weights_1))\n",
    "    return l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# main learning algorithm with one hidden layer\n",
    "# computes weights_0 and weights_1 values for the 1ts hidden layer and the output layer respectively\n",
    "# weights_0 and weights_1 are randomly assigned with values of mean 0\n",
    "# layer_0 contains the feature set data\n",
    "# layer_1 contains the hidden layer 1\n",
    "# layer_2 is our output layer\n",
    "# FeedForward is done through layers 0, 1, 2\n",
    "# weights are adjusted in the backpropagation by minimising the error\n",
    "\n",
    "\n",
    "def train(X, y, hidden_neurons=10, alpha=1, epochs=50000, dropout=False, dropout_percent=0.5):\n",
    "\n",
    "    print (\"Training with %s neurons, alpha:%s, dropout:%s %s\" % (hidden_neurons, str(alpha), dropout, dropout_percent if dropout else '') )\n",
    "    print (\"Input matrix: %sx%s    Output matrix: %sx%s\" % (len(X),len(X[0]),1, len(classes)) )\n",
    "    np.random.seed(1)\n",
    "\n",
    "    last_mean_error = 1\n",
    "    weights_0 = 2*np.random.random((len(X[0]), hidden_neurons)) - 1\n",
    "    weights_1 = 2*np.random.random((hidden_neurons, len(classes))) - 1\n",
    "\n",
    "    prev_weights_0_weight_update = np.zeros_like(weights_0)\n",
    "    prev_weights_1_weight_update = np.zeros_like(weights_1)\n",
    "\n",
    "    weights_0_direction_count = np.zeros_like(weights_0)\n",
    "    weights_1_direction_count = np.zeros_like(weights_1)\n",
    "        \n",
    "    for j in iter(range(epochs+1)):\n",
    "\n",
    "        layer_0 = X\n",
    "        layer_1 = sigmoid(np.dot(layer_0, weights_0))\n",
    "                \n",
    "        if(dropout):\n",
    "            layer_1 *= np.random.binomial(\n",
    "                [np.ones((len(X),hidden_neurons))],\n",
    "                1-dropout_percent)[0] * (1.0/(1-dropout_percent) )\n",
    "\n",
    "        layer_2 = sigmoid(np.dot(layer_1, weights_1))\n",
    "\n",
    "        # error calculation\n",
    "        layer_2_error = y - layer_2\n",
    "\n",
    "        if (j% 10000) == 0 and j > 5000:\n",
    "            # if this 10k iteration's error is greater than the last iteration, break out\n",
    "            if np.mean(np.abs(layer_2_error)) < last_mean_error:\n",
    "                print (\"delta after \"+str(j)+\" iterations:\" + str(np.mean(np.abs(layer_2_error))) )\n",
    "                last_mean_error = np.mean(np.abs(layer_2_error))\n",
    "            else:\n",
    "                print (\"break:\", np.mean(np.abs(layer_2_error)), \">\", last_mean_error )\n",
    "                break\n",
    "        \n",
    "        #Back propagation - adjust the weights based on the error\n",
    "        \n",
    "\n",
    "        layer_2_delta = layer_2_error * sigmoid_output_to_derivative(layer_2)\n",
    "        layer_1_error = layer_2_delta.dot(weights_1.T)\n",
    "        layer_1_delta = layer_1_error * sigmoid_output_to_derivative(layer_1)\n",
    "        weights_1_weight_update = (layer_1.T.dot(layer_2_delta))\n",
    "        weights_0_weight_update = (layer_0.T.dot(layer_1_delta))\n",
    "        \n",
    "        if(j > 0):\n",
    "            weights_0_direction_count += np.abs(((weights_0_weight_update > 0)+0) - ((prev_weights_0_weight_update > 0) + 0))\n",
    "            weights_1_direction_count += np.abs(((weights_1_weight_update > 0)+0) - ((prev_weights_1_weight_update > 0) + 0))        \n",
    "        \n",
    "        weights_1 += alpha * weights_1_weight_update\n",
    "        weights_0 += alpha * weights_0_weight_update\n",
    "        \n",
    "        prev_weights_0_weight_update = weights_0_weight_update\n",
    "        prev_weights_1_weight_update = weights_1_weight_update\n",
    "\n",
    "    now = datetime.datetime.now()\n",
    "\n",
    "    # copy weights into json format and save in a file\n",
    "    weights = {'weights0': weights_0.tolist(), 'weights1': weights_1.tolist(),\n",
    "               'datetime': now.strftime(\"%Y-%m-%d %H:%M\"),\n",
    "               'words': words,\n",
    "               'classes': classes\n",
    "              }\n",
    "    weights_file = \"weights.json\"\n",
    "\n",
    "    with open(weights_file, 'w') as outfile:\n",
    "        json.dump(weights, outfile, indent=4, sort_keys=True)\n",
    "    print (\"saved weights to:\", weights_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with 20 neurons, alpha:0.2, dropout:False \n",
      "Input matrix: 43x94    Output matrix: 1x2\n",
      "delta after 10000 iterations:0.00186267625198\n",
      "delta after 20000 iterations:0.00127534836096\n",
      "delta after 30000 iterations:0.0010234916989\n",
      "delta after 40000 iterations:0.000876101880876\n",
      "delta after 50000 iterations:0.000776799886483\n",
      "delta after 60000 iterations:0.000704211267516\n",
      "delta after 70000 iterations:0.000648237562044\n",
      "delta after 80000 iterations:0.000603413693284\n",
      "delta after 90000 iterations:0.000566493514191\n",
      "delta after 100000 iterations:0.000535412972966\n",
      "saved weights to: weights.json\n",
      "processing time: 16.50694751739502 seconds\n"
     ]
    }
   ],
   "source": [
    "#initiate training\n",
    "\n",
    "X = np.array(training)\n",
    "y = np.array(output)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "train(X, y, hidden_neurons=20, alpha=0.2, epochs=100000, dropout=False, dropout_percent=0.05)\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "print (\"processing time:\", elapsed_time, \"seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# probability threshold\n",
    "ERROR_THRESHOLD = 0.2\n",
    "# copy our persisted weight values into the system\n",
    "weights_file = 'weights.json' \n",
    "with open(weights_file) as data_file: \n",
    "    weights = json.load(data_file) \n",
    "    weights_0 = np.asarray(weights['weights0']) \n",
    "    weights_1 = np.asarray(weights['weights1'])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "noeat noeat\n",
      "noeat noeat\n",
      "noeat noeat\n",
      "noeat noeat\n",
      "noeat noeat\n",
      "eat eat\n",
      "eat eat\n",
      "noeat eat\n",
      "noeat eat\n",
      "eat eat\n",
      "accuracy 0.8\n"
     ]
    }
   ],
   "source": [
    "# use model to check it's performance on our cross validation data set\n",
    "# load cross validation data set into the model and check it's accuracy\n",
    "# we have 10 records set aside for cross validation, accuracy will be calculated for this specific dataset\n",
    "# for all values of alpha or different dropout percentages\n",
    "counter = 0\n",
    "pos = 0\n",
    "for sentence in validation_data:\n",
    "    if(classify(sentence['sentence'])[0][0] == sentence['class']):\n",
    "        pos = pos + 1\n",
    "    counter += 1\n",
    "    print(classify(sentence['sentence'])[0][0],sentence['class'])\n",
    "print('accuracy',pos/counter)\n",
    "   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# test set! test our model\n",
    "def classify(sentence, show_details=False):\n",
    "    results = predictusing_ann(sentence, show_details)\n",
    "\n",
    "    results = [[i,r] for i,r in enumerate(results) if r>ERROR_THRESHOLD ] \n",
    "    results.sort(key=lambda x: x[1], reverse=True) \n",
    "    return_results =[[classes[r[0]],r[1]] for r in results]\n",
    "    #print (\"%s \\n classification: %s\" % (sentence, return_results))\n",
    "    return return_results\n",
    "\n",
    "classify(\"can we go out for a sandwich\")\n",
    "classify(\"i dont want to eat anything\")\n",
    "classify(\"I will talk to you over lunch\")\n",
    "classify(\"food is fine but im not sure with french\")\n",
    "classify(\"i dont know about chinese\")\n",
    "classify(\"i dont like chinese and french\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
